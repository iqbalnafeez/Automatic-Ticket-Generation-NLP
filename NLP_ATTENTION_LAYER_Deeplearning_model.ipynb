{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ftfy import *\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "\n",
    "from utils.visualization import *\n",
    "from utils.datapreprocessing import *\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns \n",
    "\n",
    "# For Translation\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "#For Upsampling\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import Dependencies\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "# For Embedding Layer\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# For RNN Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, LSTM, Bidirectional\n",
    "from keras.layers import Dense, Dropout, Activation, Input,  Flatten\n",
    "\n",
    "# import numpy as np\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# For CNN Layer\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "import tensorflow as tf\n",
    "\n",
    "# For Text Summarization\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"dataset/final_dataframe_forDeep_Learning_Model.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complete_Description</th>\n",
       "      <th>Assignment group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>event: critical:HostName_221.company.com the v...</td>\n",
       "      <td>GRP_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when undocking pc , screen will not come back ...</td>\n",
       "      <td>GRP_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>received from: emailaddress  gentles,  i hav...</td>\n",
       "      <td>GRP_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>received from: emailaddress  hi -  the print...</td>\n",
       "      <td>GRP_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>received from: emailaddress  job Job_1424 fail...</td>\n",
       "      <td>GRP_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Complete_Description Assignment group\n",
       "0  event: critical:HostName_221.company.com the v...            GRP_1\n",
       "1  when undocking pc , screen will not come back ...            GRP_3\n",
       "2    received from: emailaddress  gentles,  i hav...            GRP_4\n",
       "3    received from: emailaddress  hi -  the print...            GRP_5\n",
       "4  received from: emailaddress  job Job_1424 fail...            GRP_6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 30, 41, 52, 63, 74, 79, 80,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36,\n",
       "       37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 64, 65, 33, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 75, 76, 77, 78,  2,  3,  0,  1,  4,  6,  5,  7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label_encoder object knows how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['Assignment group'] = label_encoder.fit_transform(df['Assignment group']) \n",
    "  \n",
    "df['Assignment group'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52229,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Assignment group']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 769, 4: 752, 2: 741, 8: 661, 30: 661, 41: 661, 52: 661, 63: 661, 74: 661, 79: 661, 80: 661, 9: 661, 10: 661, 11: 661, 12: 661, 13: 661, 14: 661, 15: 661, 16: 661, 17: 661, 18: 661, 19: 661, 20: 661, 21: 661, 22: 661, 23: 661, 24: 661, 25: 661, 26: 661, 27: 661, 28: 661, 29: 661, 31: 661, 32: 661, 34: 661, 35: 661, 36: 661, 37: 661, 38: 661, 39: 661, 40: 661, 42: 661, 43: 661, 44: 661, 45: 661, 46: 661, 47: 661, 48: 661, 49: 661, 50: 661, 51: 661, 53: 661, 54: 661, 55: 661, 56: 661, 57: 661, 58: 661, 59: 661, 60: 661, 61: 661, 62: 661, 64: 661, 65: 661, 33: 661, 66: 661, 67: 661, 68: 661, 69: 661, 70: 661, 71: 661, 72: 661, 73: 661, 75: 661, 76: 661, 77: 661, 78: 661, 0: 557, 1: 413, 6: 301, 5: 267, 7: 176})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.Complete_Description.values)\n",
    "post_seq = tokenizer.texts_to_sequences(df.Complete_Description.values)\n",
    "post_seq_padded = pad_sequences(post_seq, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(post_seq_padded, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36560, 300), (15669, 300))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36560,), (15669,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16390"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = vocab_size  \n",
    "maxlen = 200\n",
    "num_class = len(np.unique(df['Assignment group'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, h, mask=None):\n",
    "    h_shape = K.shape(h)\n",
    "    d_w, T = h_shape[0], h_shape[1]\n",
    "    \n",
    "    logits = K.dot(h, self.w)  # w^T h\n",
    "    logits = K.reshape(logits, (d_w, T))\n",
    "    alpha = K.exp(logits - K.max(logits, axis=-1, keepdims=True))  # exp\n",
    "    alpha = alpha / K.sum(alpha, axis=1, keepdims=True)  # softmax\n",
    "    r = K.sum(h * K.expand_dims(alpha), axis=1)  # r = h*alpha^T\n",
    "    h_star = K.tanh(r)  # h^* = tanh(r)\n",
    "    return h_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "\tdef __init__(self, units):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\t\tself.W1 = tf.keras.layers.Dense(units)\n",
    "\t\tself.W2 = tf.keras.layers.Dense(units)\n",
    "\t\tself.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "\tdef call(self, features, hidden):\n",
    "\t\t# hidden shape == (batch_size, hidden size)\n",
    "\t\t# hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "\t\t# we are doing this to perform addition to calculate the score\n",
    "\t\thidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\t\t  \n",
    "\t\t# score shape == (batch_size, max_length, 1)\n",
    "\t\t# we get 1 at the last axis because we are applying score to self.V\n",
    "\t\t# the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "\t\tscore = tf.nn.tanh(\n",
    "\t\t\tself.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\t\t# attention_weights shape == (batch_size, max_length, 1)\n",
    "\t\tattention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\t\t  \n",
    "\t\t# context_vector shape after sum == (batch_size, hidden_size)\n",
    "\t\tcontext_vector = attention_weights * features\n",
    "\t\tcontext_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\t\treturn context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 128)    2097920     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, None, 128)    98816       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       [(None, None, 128),  98816       bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128)          0           bi_lstm_1[0][1]                  \n",
      "                                                                 bi_lstm_1[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         ((None, 128), (None, 2591        bi_lstm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 32)           4128        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 81)           2673        dense_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,304,944\n",
      "Trainable params: 2,304,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(None,), dtype=\"int32\")\n",
    "embedded_sequences = Embedding(max_features, 128)(sequence_input)\n",
    "lstm = Bidirectional(LSTM(64, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(64, return_sequences=True, return_state=True), name=\"bi_lstm_1\")(lstm)\n",
    "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
    "state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "dropout = Dropout(0.2)(context_vector)\n",
    "dense1 = Dense(32, activation=\"relu\")(dropout)\n",
    "\n",
    "output = Dense(num_class, activation=\"sigmoid\")(dense1)\n",
    "  \n",
    "model = keras.Model(inputs=sequence_input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1828/1828 [==============================] - 536s 293ms/step - loss: 2.2663 - acc: 0.4049 - val_loss: 0.8514 - val_acc: 0.7742\n",
      "Epoch 2/7\n",
      "1828/1828 [==============================] - 530s 290ms/step - loss: 0.6000 - acc: 0.8416 - val_loss: 0.3903 - val_acc: 0.8938\n",
      "Epoch 3/7\n",
      "1828/1828 [==============================] - 523s 286ms/step - loss: 0.3094 - acc: 0.9197 - val_loss: 0.3019 - val_acc: 0.9246\n",
      "Epoch 4/7\n",
      "1828/1828 [==============================] - 489s 268ms/step - loss: 0.2026 - acc: 0.9470 - val_loss: 0.2494 - val_acc: 0.9388\n",
      "Epoch 5/7\n",
      "1828/1828 [==============================] - 529s 289ms/step - loss: 0.1476 - acc: 0.9615 - val_loss: 0.2387 - val_acc: 0.9462\n",
      "Epoch 6/7\n",
      "1828/1828 [==============================] - 482s 264ms/step - loss: 0.1077 - acc: 0.9716 - val_loss: 0.2526 - val_acc: 0.9453\n",
      "Epoch 7/7\n",
      "1828/1828 [==============================] - 459s 251ms/step - loss: 0.0898 - acc: 0.9764 - val_loss: 0.2316 - val_acc: 0.9506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e84760f48>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.fit(X_train, y_train, batch_size=20, epochs=7, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
